{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af1707a2",
   "metadata": {},
   "source": [
    "### Management and Analyisis of Physics Datasets mod. B\n",
    "# Distributed Algorithms\n",
    "### Group 4 (ü§Æ) - Bacilieri, Barbiero, Bordin, Pitteri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4379990",
   "metadata": {},
   "source": [
    "## 0 - Abstract\n",
    "A number of common algorithms used in Data Science and Machine Learning are not by default designed to take advantage of the parallelization offered by distributed systems. In this project we'll implement several K-means algorithm specifically design to take advantage of distributed systems using the PySpark API of Spark and the Cloud Veneto computing resources.\n",
    "\n",
    "Our cluster was composed by one master node and two worker nodes providing:\n",
    "- 4 physical cores, one thread each\n",
    "- ? GB of RAM\n",
    "each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a50e064",
   "metadata": {},
   "source": [
    "## 1 - Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef381ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"k-meaner\") \\\n",
    "    .config(\"spark.executor.memory\", \"1024m\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "#clear old data if rerunning\n",
    "spark.catalog.clearCache() \n",
    "for (id, rdd) in sc._jsc.getPersistentRDDs().items():\n",
    "    rdd.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffaa56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_kddcup99\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, udf, array, min as smin, lit, count, isnan, when, sum as ssum\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb39be4",
   "metadata": {},
   "source": [
    "## 2 - Dataset\n",
    "The dataset we decided to benchmark our algorithms on is the `KDDCup99` dataset.\n",
    "While we're not interested in the specific topics covered by the dataset it is still important to describe its key features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ed42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdd = spark.createDataFrame(fetch_kddcup99(as_frame=True)[\"frame\"])\n",
    "\n",
    "kdd.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc29d39",
   "metadata": {},
   "source": [
    "As we can see `kdd` is a tuple where the first element is an array of data points (the features), while the second element is filled with the corresponding labels ‚Äì i.e., the attack types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822322d",
   "metadata": {},
   "source": [
    "### Pre Processing\n",
    "\n",
    "- Remove the first three ‚Äúcategorical‚Äù columns, and keep only the numerical ones. \n",
    "- Remove two columns that happen to be all zeros.\n",
    "- Rescale everything in [0-1] range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90a697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdd = kdd.drop(\"protocol_type\")\n",
    "kdd = kdd.drop(\"service\") \n",
    "kdd = kdd.drop(\"flag\")\n",
    "\n",
    "kdd.createOrReplaceTempView(\"kdd_table\")\n",
    "\n",
    "for c in kdd.columns:\n",
    "    try:\n",
    "        limits = spark.sql(\"SELECT min(\" + c + \"), max(\" + c + \") FROM kdd_table\").collect()\n",
    "        #mx = kdd.groupby().max(c).first().asDict()['max('+c+')']\n",
    "        #mn = kdd.groupby().min(c).first().asDict()['min('+c+')']\n",
    "        mn, mx = limits[0][0], limits[0][1]\n",
    "        \n",
    "        if mn==mx:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        kdd = kdd.withColumn(c, (col(c)-mn)/(mx-mn) )\n",
    "        \n",
    "        #print(limits[0][0], limits[0][1])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "kdd = kdd.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3854f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdd.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca4be8",
   "metadata": {},
   "source": [
    "### Attack types distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763ce39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = spark.sql(\"SELECT labels, count(1) FROM kdd_table GROUP BY labels\").collect()\n",
    "\n",
    "attacks = np.array([r[0] for r in res])\n",
    "counts = np.array([r[1] for r in res])\n",
    "attack_sort = np.argsort(counts)\n",
    "attacks = attacks[attack_sort]\n",
    "counts = counts[attack_sort]\n",
    "\n",
    "ypos = np.arange(len(attacks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7cc116",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(ypos, np.log10(counts), facecolor=\"gold\")\n",
    "plt.yticks(ypos, labels=[a.decode(\"utf-8\")[0:-1] for a in attacks])\n",
    "plt.xlabel(r\"$\\log_{10}(\\mathrm{count})$\")\n",
    "plt.ylabel(\"Attack type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d44ef30",
   "metadata": {},
   "source": [
    "Attack types vary significantly in frequency so we will have to keep this in mind during later clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440ffe97",
   "metadata": {},
   "source": [
    "## 3 - Lloyd's Algorithm\n",
    "The K means algorithm is widely used in Machine Learing clustering applications due to its simple implementation which roughly follows these steps.\n",
    "\n",
    "- Start from a set of points as centers for the clusters\n",
    "- Classify data points based on closest cluster center\n",
    "- Set new cluster center as the mean of clustered points and repeat until convergence or max iterations\n",
    "\n",
    "This algorithms is known as Lloyd's algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55558d6b",
   "metadata": {},
   "source": [
    "### Prep work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeade898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x,y):\n",
    "    try:\n",
    "        #return (((np.array(x)-y)**2).sum())**0.5\n",
    "        return np.linalg.norm(np.array(x)-np.array(y))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return ncols\n",
    "\n",
    "def argcomp(comp,func, arr, *params):\n",
    "    res = [func(arr[i], *params) for i in range(len(arr))]\n",
    "    #print(res)\n",
    "    return res.index(comp(res))\n",
    "\n",
    "index_udf = udf(lambda row:attacks.tolist().index(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876b631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored_cols = [\"labels\", \"centr\", \"cost\", \"p\", \"w\"]\n",
    "\n",
    "kdd = kdd.withColumn(\"labels\", index_udf(col(\"labels\")).cast(\"int\"))\n",
    "kdd = kdd.withColumn(\"centr\", lit(-1))\n",
    "kdd = kdd.withColumn(\"cost\", lit(0.))\n",
    "kdd = kdd.withColumn(\"p\", lit(0.))\n",
    "\n",
    "ncols = len([c for c in kdd.columns if c not in ignored_cols])\n",
    "\n",
    "kdd.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aac702",
   "metadata": {},
   "source": [
    "### Lloyd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf114ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(data, centers, max_iter=100, weighted=False, local_centr=False, return_cost=False):\n",
    "    count = 0\n",
    "    \n",
    "    \n",
    "    cols = data.columns\n",
    "    \n",
    "    cost = float(\"inf\")\n",
    "    \n",
    "    while count<=max_iter:\n",
    "        if local_centr:\n",
    "            cc = centers\n",
    "            local_centr = False\n",
    "        else:\n",
    "            cc = centers.collect()\n",
    "        #print(cc)\n",
    "        \n",
    "        ## set center affiliation\n",
    "        argmindist_udf = udf(lambda row: argcomp(min,dist,cc,row), IntegerType())\n",
    "        data = data.withColumn(\"centr\", argmindist_udf(array([c for c in cols if c not in ignored_cols])))\n",
    "        \n",
    "        ## calculate new centers\n",
    "        if not weighted:\n",
    "            centers = data.groupBy(\"centr\").mean().select(*[col(\"avg(\"+c+\")\") for c in data.columns if c not in ignored_cols])\n",
    "        else:\n",
    "            centers = data.rdd.map(\n",
    "                lambda x: (x[\"centr\"],(np.array([x[c] for c in cols if c not in ignored_cols]),x[\"w\"]))\n",
    "            ).reduceByKey(\n",
    "                lambda x,y: ((x[0]*x[1]+y[0]*y[1])/(x[1]+y[1]), x[1]+y[1])\n",
    "            ).map(lambda x: x[1][0])\n",
    "        \n",
    "        get_cost = udf(lambda row: str(min([dist(row, cen)**2 for cen in cc])))\n",
    "        \n",
    "        data = data.withColumn(\"cost\", get_cost(array([c for c in cols if c not in ignored_cols])).cast(\"double\"))\n",
    "        newcost = data.select(\"cost\").groupBy().sum().collect()[0][0]\n",
    "        \n",
    "        count+=1\n",
    "        if newcost/cost>0.99:\n",
    "            break\n",
    "        \n",
    "        cost=newcost\n",
    "            \n",
    "    if return_cost:\n",
    "        return data, centers, cost\n",
    "    else:\n",
    "        return data, centers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9cc110",
   "metadata": {},
   "source": [
    "## 4 - Centroids initialization\n",
    "In order to optimize performance of the Lloyd's algorithm proper centroids initialization is paramount. Here we'll propose three initialization methods\n",
    "\n",
    "### JSI (Just Stupid Initialization)\n",
    "The most general way to decide the first centroids is to initialize them randomly all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4dd5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_init(k, mn=0, mx=1):\n",
    "    return pd.DataFrame(np.random.random(size=(k, ncols))*(mx-mn) - mn)\n",
    "\n",
    "unif_data, _ = kmeans(kdd, uniform_init(len(attacks)), local_centr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80b1c3d",
   "metadata": {},
   "source": [
    "This simple algorithm is, however, often times not the best choice for performance since clusters can be chosen in sub-optimal configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc548ba",
   "metadata": {},
   "source": [
    "### Kmeans++\n",
    "On the other end of the spectrum we have `Kmeans++`. \n",
    "This algorithms initializes centroids sequentially one by one with the following logic\n",
    "- assign the first center randomly\n",
    "- each next centroid is assigned with probability proportional to the distance from each other centroid\n",
    "\n",
    "This assures centroids evenly spaced across the feature space but has the drawback of being inherently sequential in nature and thus cannot benefit from a parallel cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b607587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kplus_init(data, n_clusters):\n",
    "    c = np.empty((n_clusters, data.shape[1]))\n",
    "    c[0] = data[np.random.choice(data.shape[0])]\n",
    "\n",
    "    for i in range(1, n_clusters):\n",
    "        distances = np.sum((data[:, np.newaxis] - c[:i])**2, axis=-1)\n",
    "        closest = np.argmin(distances, axis=-1)\n",
    "        \n",
    "        distances = distances[np.arange(len(distances)), closest]\n",
    "        \n",
    "        distances = distances / np.sum(distances)\n",
    "        c_idx = choice(np.arange(len(distances)), p=distances)\n",
    "        c[i] = data[c_idx]\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d05444",
   "metadata": {},
   "source": [
    "### Kmeans||\n",
    "The middle ground between the two initialization methods is to be found in `Kmeans||`. Centroids here are initialized in groups (thus parallelizable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c0a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_parallel(data, k, l):\n",
    "    centr = data.select(array([c for c in data.columns if c not in ignored_cols])).rdd.takeSample(True,2)\n",
    "    #define get_cost using udf on min(dist(row,centr[i]))\n",
    "    get_cost = udf(lambda row: str(min([dist(row, cen)**2 for cen in centr])))\n",
    "    \n",
    "    data = data.withColumn(\"cost\", get_cost(array([c for c in data.columns if c not in ignored_cols])).cast(\"double\"))\n",
    "    \n",
    "    cost = data.select(\"cost\").groupBy().sum().collect()[0][0]\n",
    "    \n",
    "    for i in range(int(np.log(cost))):\n",
    "        #re-define get_cost using udf on min(dist(row,centr[i]))\n",
    "        get_cost = udf(lambda row: str(min([dist(row, cen)**2 for cen in centr])))\n",
    "        data = data.withColumn(\"cost\", get_cost(array([c for c in data.columns if c not in ignored_cols])).cast(\"double\"))\n",
    "        cost = data.select(\"cost\").groupBy().sum().collect()[0][0]\n",
    "        \n",
    "        data = data.withColumn(\"p\", udf(lambda row: np.random.random())(col(\"cost\")))\n",
    "        \n",
    "        centr.extend( data.where( data[\"cost\"] * l > cost * data[\"p\"]).select(array([c for c in data.columns if c not in ignored_cols])).collect() )\n",
    "    \n",
    "    ## search for possible identical centroids ? \n",
    "    #should be taken care of thanks to the weighting, I think\n",
    "    \n",
    "    if len(centr) > k: \n",
    "        #get weights\n",
    "        data, _ = kmeans(data, centr, max_iter=0, local_centr=True)\n",
    "        counts = data.groupBy(\"centr\").count().collect()\n",
    "        counts = sorted(counts)\n",
    "        \n",
    "        #parallelize the centroid dataset\n",
    "        cdata = spark.createDataFrame(pd.DataFrame([[*centr[i][0], counts[i][1]] for i in range(len(centr))], columns=(0,1,\"w\")))\n",
    "        cdata = cdata.withColumn(\"w\", col(\"w\").cast(\"double\"))\n",
    "        \n",
    "        ## basically kmeans++ on the centroids as init for the weighted k-means\n",
    "        c = [centr[np.random.randint(len(centr))]]\n",
    "        for i in range(k-1):\n",
    "            costs = [min([dist(centr[j], c[k])**2 for k in range(len(c))]) for j in range(len(centr))]\n",
    "            p = np.cumsum(costs)\n",
    "            p/= p[-1]\n",
    "            c.append(centr[(p>np.random.random()).tolist().index(True)])\n",
    "        \n",
    "        \n",
    "        _, centr = kmeans(cdata, c, local_centr=True, weighted=True)\n",
    "        centr = centr.collect()\n",
    "    return centr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8868c5",
   "metadata": {},
   "source": [
    "## 5 - Mini Batch Kmeans\n",
    "Another way to exploit parallel architectures for a Kmeans clustering task is through the usage of mini batch Kmeans (MBK).\n",
    "Unlike the previous methods MBK performs iterative clusterings for small batches of the dataset and updates centroids positions based on a learning rate. This is generally slower compared to previous methods but its simple and flexible implementation along with the excellent scaling w.r.t. the size of the dataset (the algo never looks at the whole dataset) makes it a viable alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a859a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_b(data,batch_size,n_cent,l_rate=0.05,max_it=None):\n",
    "    if max_it is None:\n",
    "        max_it = int(2/batch_size) #standard (is it?) amount of iterations\n",
    "    \n",
    "    #first centroids initialization\n",
    "    initialCentroids = data.sample(batch_size).limit(n_cent).select(*[col(c) for c in data.columns if c not in ignored_cols])\n",
    "    initialCentroids = np.array(initialCentroids.collect())\n",
    "    \n",
    "    miniBatch = data.sample(batch_size)\n",
    "    _, _, cost = kmeans(miniBatch,initialCentroids, local_centr=True, max_iter=0, return_cost=True)\n",
    "    \n",
    "    #centroids update\n",
    "    for _ in range(max_it):\n",
    "        miniBatch = data.sample(batch_size)\n",
    "        \n",
    "        _,newCentroids, newcost = kmeans(miniBatch,initialCentroids, local_centr=True, max_iter=5, return_cost=True)\n",
    "        \n",
    "        newCentroids = np.array(newCentroids.collect())\n",
    "        \n",
    "        #sort centroids to minimize reciprocal distance by finding the optimal permutation on newCentroids\n",
    "        \n",
    "        distances = np.linalg.norm(initialCentroids[:, np.newaxis] - newCentroids, axis=2)\n",
    "        permutation = np.argmin(np.sum(distances, axis=0))\n",
    "        newCentroids = newCentroids[permutation]\n",
    "        \n",
    "        initialCentroids = (1-l_rate) * initialCentroids + newCentroids * l_rate\n",
    "        \n",
    "        if newcost/cost>0.99:\n",
    "            break\n",
    "        else:\n",
    "            cost=newcost\n",
    "\n",
    "    return initialCentroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821a4c68",
   "metadata": {},
   "source": [
    "The middle permutation was needed since there's no guarantee that the returning list of centroids will be in the same order as the original one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f8f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "newcenters = mini_b(kdd,0.2,len(attacks),0.05,10)\n",
    "mini_data,_=kmeans(kdd,newcenters, local_centr=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
