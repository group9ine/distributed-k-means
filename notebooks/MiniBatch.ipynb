{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2609b750",
   "metadata": {},
   "source": [
    "## Minibatch k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6037b6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_kddcup99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48f9f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, udf, array, min as smin, lit, count, isnan, when, sum as ssum\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693c7cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"k-meaner\") \\\n",
    "    .config(\"spark.executor.memory\", \"1024m\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "#clear old data if rerunning\n",
    "spark.catalog.clearCache() \n",
    "for (id, rdd) in sc._jsc.getPersistentRDDs().items():\n",
    "    rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75917759",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde3376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdd = spark.createDataFrame(fetch_kddcup99(as_frame=True)[\"frame\"])\n",
    "kdd = kdd.drop(\"protocol_type\")\n",
    "kdd = kdd.drop(\"service\") \n",
    "kdd = kdd.drop(\"flag\")\n",
    "kdd.createOrReplaceTempView(\"kdd_table\")\n",
    "\n",
    "for c in kdd.columns:\n",
    "    try:\n",
    "        limits = spark.sql(\"SELECT min(\" + c + \"), max(\" + c + \") FROM kdd_table\").collect()\n",
    "        mn, mx = limits[0][0], limits[0][1]\n",
    "        \n",
    "        if mn==mx:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        kdd = kdd.withColumn(c, (col(c)-mn)/(mx-mn) )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "kdd = kdd.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f560d1c9",
   "metadata": {},
   "source": [
    "### Lloyd algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2593ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_udf = udf(lambda row:attacks.tolist().index(row) )\n",
    "kdd = kdd.withColumn(\"labels\", index_udf(col(\"labels\")).cast(\"int\"))\n",
    "\n",
    "ignored_cols = [\"labels\", \"centr\", \"cost\", \"p\"]\n",
    "\n",
    "kdd = kdd.withColumn(\"centr\", lit(-1))\n",
    "kdd = kdd.withColumn(\"cost\", lit(0.))\n",
    "kdd = kdd.withColumn(\"p\", lit(0.))\n",
    "ncols = len(kdd.columns) - len(ignored_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x,y):\n",
    "    try:\n",
    "        #return (((np.array(x)-y)**2).sum())**0.5\n",
    "        return np.linalg.norm(np.array(x)-np.array(y))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return ncols\n",
    "\n",
    "def argcomp(comp,func, arr, *params):\n",
    "    res = [func(arr[i], *params) for i in range(len(arr))]\n",
    "    #print(res)\n",
    "    return res.index(comp(res))\n",
    "\n",
    "#udist=udf(dist, FloatType())\n",
    "\n",
    "#distance_udf = udf(lambda x,y:  np.linalg.norm(x-y), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fb7974",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### actual k-means \n",
    "def kmeans(data, centers, max_iter=10, weighted=False, local_centr=False):\n",
    "    count = 0\n",
    "    if weighted and \"w\" not in ignored_cols:\n",
    "        ignored_cols.append(\"w\")\n",
    "    while True:\n",
    "        if local_centr:\n",
    "            cc = centers\n",
    "            local_centr = False\n",
    "        else:\n",
    "            cc = centers.collect()\n",
    "        #print(cc)\n",
    "        \n",
    "        argmindist_udf = udf(lambda row: argcomp(min,dist,cc,row), IntegerType())\n",
    "        \n",
    "        data = data.withColumn(\"centr\", argmindist_udf(array([c for c in data.columns if c not in ignored_cols])))\n",
    "        \n",
    "        if not weighted:\n",
    "            newcenters = data.groupBy(\"centr\").mean().select(*[col(\"avg(\"+c+\")\") for c in data.columns if c not in ignored_cols])\n",
    "        else:\n",
    "            #newcenters = data.groupBy(\"centr\").agg(ssum(array([c for c in data.columns if c not in ignored_cols]) * col(\"w\"))/ssum(col(\"w\")))\n",
    "            newcenters = data.rdd.map(\n",
    "                lambda x: (x[\"centr\"],(x[[c for c in data.columns if c not in ignored_cols]],x[\"w\"]))\n",
    "            ).reduceByKey(\n",
    "                lambda x,y: ((x[0]*x[1]+y[0]*y[1])/x[1]+y[1], x[1]+y[1])\n",
    "            ).map(lambda x: x[1][0]).collect()\n",
    "        \n",
    "        centers = newcenters\n",
    "        \n",
    "        count+=1\n",
    "        if count>max_iter:#(newcenters-centers).mean() < 0.01:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    return data, centers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62811b99",
   "metadata": {},
   "source": [
    "### K-means mini batch\n",
    "Since the idea is to never directly look at the whole dataset we will draw a random sample from the data to act as the starting centers. We will then compute new center using k means on a mini batch of the original data, then the new centers for the following iteration will be computed as old_c+l.rate * new_c until convergence or max iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adef9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_b(data,batch_size,l_rate,max_it,n_cent):\n",
    "    #first centroids initialization\n",
    "    initialCentroids = data.sample(batch_size).limit(n_cent).select(*[col(c) for c in data.columns if c not in ignored_cols])\n",
    "    initialCentroids = np.array(initialCentroids.collect())\n",
    "    \n",
    "    #centroids update\n",
    "    \n",
    "    for _ in range(max_it):\n",
    "        miniBatch = data.sample(batch_size)\n",
    "        \n",
    "        _,newCentroids = kmeans(miniBatch,initialCentroids)\n",
    "        \n",
    "        newCentroids = np.array(newCentroids.collect())\n",
    "        \n",
    "        #sort centroids to minimize reciprocal distance by finding the optimal permutation on newCentroids\n",
    "        \n",
    "        distances = np.linalg.norm(initialCentroids[:, np.newaxis] - newCentroids, axis=2)\n",
    "        permutation = np.argmin(np.sum(distances, axis=0))\n",
    "        newCentroids = newCentroids2[permutation]\n",
    "        \n",
    "        initialCentroids = (1-l_rate) * initialCentroids + newCentroids * l_rate\n",
    "\n",
    "    return initialCentroids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
