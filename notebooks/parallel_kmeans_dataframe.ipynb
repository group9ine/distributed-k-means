{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.9 MB 4.5 MB/s eta 0:00:01    |████████████▍                   | 4.2 MB 4.5 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting scipy>=1.5.0\n",
      "  Downloading scipy-1.11.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 36.5 MB 15.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.9/site-packages (from scikit-learn) (1.23.5)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.3.1-py3-none-any.whl (301 kB)\n",
      "\u001b[K     |████████████████████████████████| 301 kB 18.4 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.1 scikit-learn-1.3.0 scipy-1.11.1 threadpoolctl-3.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (1.11.1)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.9/site-packages (from scipy) (1.23.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_kddcup99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, array, min as smin\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/06 07:17:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"k-meaner\") \\\n",
    "    .config(\"spark.executor.memory\", \"1024m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the `kddcup99` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/usr/bin/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "kdd = spark.createDataFrame(fetch_kddcup99(as_frame=True)[\"frame\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "We remove the first three “categorical” columns, and keep only the numerical ones. Then we rescale those to a [0, 1] range. Also, we remove two columns that happen to be all zeros. After all, this shouldn’t affect the clustering afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdd.createOrReplaceTempView(\"kdd_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in kdd.columns:\n",
    "    try:\n",
    "        limits = spark.sql(\"SELECT min(\" + c + \"), max(\" + c + \") FROM kdd_table\").collect()\n",
    "        #mx = kdd.groupby().max(c).first().asDict()['max('+c+')']\n",
    "        #mn = kdd.groupby().min(c).first().asDict()['min('+c+')']\n",
    "        mn, mx = limits[0][0], limits[0][1]\n",
    "        kdd = kdd.withColumn(c, (col(c)-mn)/(mx-mn) )\n",
    "        #print(limits[0][0], limits[0][1])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "#kdd.take(1)\n",
    "#help(kdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assembler = VectorAssembler(inputCols=[c for c in kdd.columns if c not in [\"protocol_type\", \"service\", \"flag\", \"labels\"]], outputCol=\"features\")\n",
    "assembled_df = assembler.transform(kdd)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(assembled_df)\n",
    "normalized_data = scaler_model.transform(assembled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the distribution of attack types…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"targets = kdd[1]\n",
    "\n",
    "attacks, counts = np.unique(targets, return_counts=True)\n",
    "ypos = np.arange(len(attacks))\n",
    "\n",
    "attack_sort = np.argsort(counts)\n",
    "attacks = attacks[attack_sort]\n",
    "counts = counts[attack_sort]\n",
    "\"\"\"\n",
    "res = spark.sql(\"SELECT labels, count(1) FROM kdd_table GROUP BY labels\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks = np.array([r[0] for r in res])\n",
    "counts = np.array([r[1] for r in res])\n",
    "attack_sort = np.argsort(counts)\n",
    "attacks = attacks[attack_sort]\n",
    "counts = counts[attack_sort]\n",
    "\n",
    "ypos = np.arange(len(attacks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(ypos, np.log10(counts), facecolor=\"gold\")\n",
    "plt.yticks(ypos, labels=[a.decode(\"utf-8\")[0:-1] for a in attacks])\n",
    "plt.xlabel(r\"$\\log_{10}(\\mathrm{count})$\")\n",
    "plt.ylabel(\"Attack type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start on k-means basic algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored_cols = [\"protocol_type\", \"service\", \"flag\", \"labels\", \"centr\"]\n",
    "ncols = len(kdd.columns) - len(ignored_cols)\n",
    "\n",
    "def dist(x,y):\n",
    "    try:\n",
    "        return (((np.array(x)-y)**2).sum())**0.5\n",
    "    except:\n",
    "        return ncols\n",
    "\n",
    "def argcomp(comp,func, arr, *params):\n",
    "    res = [func(arr[i], *params) for i in range(len(arr))]\n",
    "    #print(res)\n",
    "    return res.index(comp(res))\n",
    "\n",
    "#udist=udf(dist, FloatType())\n",
    "\n",
    "#distance_udf = udf(lambda x,y:  np.linalg.norm(x-y), FloatType())\n",
    "\n",
    "###################################### actual k-means \n",
    "def kmeans(data, centers):\n",
    "    count = 0\n",
    "    while True:\n",
    "        #data = data.map(lambda x: [argcomp(min,dist,centers,x), x[1]])\n",
    "        #newcenters = data.reduceByKey(lambda x,y: np.mean([x,y], axis=1))\n",
    "        cc = centers.collect()\n",
    "        #print(cc)\n",
    "        argmindist_udf = udf(lambda row: argcomp(min,dist,cc,row), IntegerType())\n",
    "        \n",
    "        #data = data.withColumn(\"centr\", argcomp(min, dist, centers.collect(), [data[c] for c in data.columns if c not in ignored_cols]))\n",
    "        data = data.withColumn(\"centr\", argmindist_udf(array([c for c in data.columns if c not in ignored_cols])))\n",
    "        \n",
    "        centr = data.select(\"centr\").where(data.centr!=0).collect()\n",
    "        if len(centr) == 0:\n",
    "            print(\"!!!! all zero !!!!\")\n",
    "        \n",
    "        newcenters = data.groupBy(\"centr\").mean().select(*[col(\"avg(\"+c+\")\") for c in data.columns if c not in ignored_cols])\n",
    "        \n",
    "        print(count)\n",
    "        count+=1\n",
    "        if count>1:#(newcenters-centers).mean() < 0.01:\n",
    "            break\n",
    "        else:\n",
    "            centers = newcenters\n",
    "    \n",
    "    return data, centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = pd.DataFrame(np.random.random(size=(ncols,len(attacks))))\n",
    "centers = spark.createDataFrame(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldata, newcenters = kmeans(kdd, centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newcenters.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centr = finaldata.select(\"centr\").collect()\n",
    "\n",
    "for i in range(len(centr)):\n",
    "    if(centr[i][0] != 0):\n",
    "        print(centr[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame(np.random.random(size=(30,2))*2-1)\n",
    "test_data[\"labels\"]=[*([0]*30)]\n",
    "test_data[\"protocol_type\"]=[*([\"http\"]*30)]\n",
    "test_data[\"protocol_type\"][3]=\"smtp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"labels\"][(test_data[0]<0) & (test_data[1]>0)]=1\n",
    "test_data[\"labels\"][(test_data[0]<0) & (test_data[1]<0)]=2\n",
    "test_data[\"labels\"][(test_data[0]>0) & (test_data[1]<0)]=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = spark.createDataFrame(test_data)\n",
    "#test_cent = spark.createDataFrame(pd.DataFrame(np.array([[1,1], [-1,1], [-1,-1], [1,-1]])))\n",
    "test_cent = spark.createDataFrame(pd.DataFrame(np.random.random(size=(4,2))*2-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tdata, res_tcent = kmeans(test_data, test_cent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tcent.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = res_tdata.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(d, len(d))\n",
    "mat = [[0 for i in range(4)] for j in range(4)]\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        for k in range(len(d)):\n",
    "            mat[i][j] += ((d[k][\"labels\"] == i) and (d[k][\"centr\"] == j))\n",
    "mat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
